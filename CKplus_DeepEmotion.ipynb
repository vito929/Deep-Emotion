{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy  as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from data_loaders import Plain_Dataset, eval_data_dataloader\n",
    "from deep_emotion import Deep_Emotion\n",
    "from generate_data import Generate_data\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "data_path = 'ck_new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(epochs,train_loader,val_loader,criterion,optmizer,device):\n",
    "    '''\n",
    "    Training Loop\n",
    "    '''\n",
    "    print(\"===================================Start Training===================================\")\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        # Train the model  #\n",
    "        net.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optmizer.zero_grad()\n",
    "            outputs = net(data)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optmizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs,1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        #validate the model#\n",
    "        net.eval()\n",
    "        for data,labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            val_outputs = net(data)\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            validation_loss += val_loss.item()\n",
    "            _, val_preds = torch.max(val_outputs,1)\n",
    "            val_correct += torch.sum(val_preds == labels.data)\n",
    "\n",
    "        train_loss = train_loss/len(train_dataset)\n",
    "        train_acc = train_correct.double() / len(train_dataset)\n",
    "        validation_loss =  validation_loss / len(validation_dataset)\n",
    "        val_acc = val_correct.double() / len(validation_dataset)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
    "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
    "\n",
    "    torch.save(net.state_dict(),'deep_emotion-ckplus-{}-{}-{}.pt'.format(epochs,batchsize,lr))\n",
    "    print(\"===================================Training Finished===================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create train,vali,test csv file\n",
    "70%, 10%, 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "with open('ck_plus.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    rows = [row for row in reader]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_each = 9\n",
    "\n",
    "random_array = np.arange(len(rows))\n",
    "cnt_an = 0\n",
    "cnt_di = 0\n",
    "cnt_fe = 0\n",
    "cnt_ha = 0\n",
    "cnt_sa = 0\n",
    "cnt_su = 0\n",
    "cnt_ne = 0\n",
    "loop = True\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "st = []\n",
    "st_lbl = []\n",
    "idx = []\n",
    "while loop:\n",
    "    temp = np.random.choice(random_array)\n",
    "    item = rows[temp][0]\n",
    "    lbl = int(rows[temp][1])\n",
    "    if item not in st:\n",
    "        \n",
    "        if lbl == 0 and cnt_an<num_each :\n",
    "            cnt_an = cnt_an+1\n",
    "            st_lbl.append(lbl)\n",
    "            st.append(item)\n",
    "            idx.append(temp)\n",
    "        elif lbl==1 and cnt_di<num_each:\n",
    "            cnt_di = cnt_di+1\n",
    "            st_lbl.append(lbl)\n",
    "            st.append(item)\n",
    "            idx.append(temp)\n",
    "        elif lbl==2 and cnt_fe<num_each:\n",
    "            cnt_fe = cnt_fe+1\n",
    "            st_lbl.append(lbl)\n",
    "            st.append(item)\n",
    "            idx.append(temp)\n",
    "        elif lbl==3 and cnt_ha<num_each:\n",
    "            cnt_ha = cnt_ha +1\n",
    "            st_lbl.append(lbl)\n",
    "            st.append(item)\n",
    "            idx.append(temp)\n",
    "        elif lbl == 4 and cnt_sa<num_each:\n",
    "            cnt_sa = cnt_sa +1\n",
    "            st_lbl.append(lbl)\n",
    "            st.append(item)\n",
    "            idx.append(temp)\n",
    "        elif lbl==5 and cnt_su<num_each:\n",
    "            cnt_su = cnt_su+1\n",
    "            st_lbl.append(lbl)\n",
    "            st.append(item)\n",
    "            idx.append(temp)\n",
    "        elif lbl==6 and cnt_ne<num_each:\n",
    "            cnt_ne = cnt_ne+1\n",
    "            st_lbl.append(lbl)\n",
    "            st.append(item)\n",
    "            idx.append(temp)\n",
    "\n",
    "#         print(len(st))\n",
    "        if cnt_an==num_each and cnt_di==num_each and cnt_fe==num_each and cnt_ha==num_each and cnt_sa==num_each and cnt_su==num_each and cnt_ne==num_each:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_ckplus.csv', 'w', encoding='UTF8',newline='') as test_ckplus:\n",
    "    test_writer = csv.writer(test_ckplus)\n",
    "    for i in range(len(rows)):\n",
    "        if i in idx:\n",
    "            test_writer.writerow(rows[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vali.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = 32\n",
    "idx_val = []\n",
    "cnt_val = 0\n",
    "t=0\n",
    "while True:\n",
    "    t = t+1\n",
    "    temp = np.random.choice(random_array)\n",
    "    if temp not in idx:\n",
    "        if cnt_val>=num_val:\n",
    "            break\n",
    "        cnt_val = cnt_val+1\n",
    "        idx_val.append(temp)\n",
    "#     print(t,cnt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vali_ckplus.csv', 'w', encoding='UTF8',newline='') as vali_ckplus:\n",
    "    vali_writer = csv.writer(vali_ckplus)\n",
    "    for i in range(len(rows)):\n",
    "        if i in idx_val:\n",
    "            vali_writer.writerow(rows[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_ckplus.csv', 'w', encoding='UTF8',newline='') as train_ckplus:\n",
    "    train_writer = csv.writer(train_ckplus)\n",
    "    for i in range(len(rows)):\n",
    "        if i not in idx and i not in idx_val:\n",
    "            train_writer.writerow(rows[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jaffe_Dataset(Dataset):\n",
    "    def __init__(self,csv_file,img_dir,datatype,transform):\n",
    "        with open(csv_file,'r') as csvfile:\n",
    "            rd = csv.reader(csvfile)\n",
    "            self.data = [row for row in rd]\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.datatype = datatype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "#         img = Image.open(self.img_dir+self.data[idx][0])\n",
    "\n",
    "        img = cv2.imread(self.img_dir+self.data[idx][0],0)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#         print(img.shape)\n",
    "        lables = np.array(int(self.data[idx][1]))\n",
    "        lables = torch.from_numpy(lables).long()\n",
    "\n",
    "        if self.transform :\n",
    "            img = self.transform(img)\n",
    "        return img,lables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincsv_file = 'train_ckplus.csv'\n",
    "validationcsv_file = 'vali_ckplus.csv'\n",
    "train_img_dir = 'ck_new/'\n",
    "validation_img_dir = 'ck_new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 4\n",
    "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "# transformation= transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset= Jaffe_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
    "validation_dataset= Jaffe_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
    "train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
    "val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "net = Deep_Emotion()\n",
    "net.to(device)\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optmizer= optim.Adam(net.parameters(),lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================Start Training===================================\n",
      "Epoch: 1 \tTraining Loss: 0.42105620 \tValidation Loss 0.42464104 \tTraining Acuuarcy 39.655% \tValidation Acuuarcy 50.000%\n",
      "Epoch: 2 \tTraining Loss: 0.28998634 \tValidation Loss 0.30538505 \tTraining Acuuarcy 63.362% \tValidation Acuuarcy 56.250%\n",
      "Epoch: 3 \tTraining Loss: 0.21413986 \tValidation Loss 0.27801306 \tTraining Acuuarcy 72.414% \tValidation Acuuarcy 65.625%\n",
      "Epoch: 4 \tTraining Loss: 0.14461304 \tValidation Loss 0.28765207 \tTraining Acuuarcy 82.328% \tValidation Acuuarcy 62.500%\n",
      "Epoch: 5 \tTraining Loss: 0.13017361 \tValidation Loss 0.20455906 \tTraining Acuuarcy 83.190% \tValidation Acuuarcy 65.625%\n",
      "Epoch: 6 \tTraining Loss: 0.10906308 \tValidation Loss 0.23160118 \tTraining Acuuarcy 87.500% \tValidation Acuuarcy 65.625%\n",
      "Epoch: 7 \tTraining Loss: 0.08493465 \tValidation Loss 0.20749995 \tTraining Acuuarcy 89.224% \tValidation Acuuarcy 71.875%\n",
      "Epoch: 8 \tTraining Loss: 0.08826700 \tValidation Loss 0.21346084 \tTraining Acuuarcy 86.638% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 9 \tTraining Loss: 0.06266985 \tValidation Loss 0.35908041 \tTraining Acuuarcy 91.810% \tValidation Acuuarcy 62.500%\n",
      "Epoch: 10 \tTraining Loss: 0.05305985 \tValidation Loss 0.21895810 \tTraining Acuuarcy 93.103% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 11 \tTraining Loss: 0.05018909 \tValidation Loss 0.18551926 \tTraining Acuuarcy 93.966% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 12 \tTraining Loss: 0.04315876 \tValidation Loss 0.24663720 \tTraining Acuuarcy 95.259% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 13 \tTraining Loss: 0.05414883 \tValidation Loss 0.24408844 \tTraining Acuuarcy 93.966% \tValidation Acuuarcy 71.875%\n",
      "Epoch: 14 \tTraining Loss: 0.05024245 \tValidation Loss 0.17431872 \tTraining Acuuarcy 92.241% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 15 \tTraining Loss: 0.04355967 \tValidation Loss 0.19337559 \tTraining Acuuarcy 93.966% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 16 \tTraining Loss: 0.03856559 \tValidation Loss 0.34311048 \tTraining Acuuarcy 95.690% \tValidation Acuuarcy 62.500%\n",
      "Epoch: 17 \tTraining Loss: 0.04287923 \tValidation Loss 0.20794562 \tTraining Acuuarcy 93.966% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 18 \tTraining Loss: 0.02839191 \tValidation Loss 0.15983138 \tTraining Acuuarcy 96.121% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 19 \tTraining Loss: 0.04885064 \tValidation Loss 0.24054155 \tTraining Acuuarcy 91.810% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 20 \tTraining Loss: 0.02875395 \tValidation Loss 0.21697408 \tTraining Acuuarcy 97.414% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 21 \tTraining Loss: 0.03597629 \tValidation Loss 0.20500562 \tTraining Acuuarcy 95.259% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 22 \tTraining Loss: 0.01635810 \tValidation Loss 0.21322011 \tTraining Acuuarcy 98.276% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 23 \tTraining Loss: 0.02520045 \tValidation Loss 0.36874930 \tTraining Acuuarcy 96.983% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 24 \tTraining Loss: 0.02936022 \tValidation Loss 0.26206457 \tTraining Acuuarcy 96.121% \tValidation Acuuarcy 71.875%\n",
      "Epoch: 25 \tTraining Loss: 0.02711530 \tValidation Loss 0.25042217 \tTraining Acuuarcy 96.552% \tValidation Acuuarcy 71.875%\n",
      "Epoch: 26 \tTraining Loss: 0.00899887 \tValidation Loss 0.22521169 \tTraining Acuuarcy 100.000% \tValidation Acuuarcy 71.875%\n",
      "Epoch: 27 \tTraining Loss: 0.01826445 \tValidation Loss 0.26452769 \tTraining Acuuarcy 98.276% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 28 \tTraining Loss: 0.01892460 \tValidation Loss 0.33900959 \tTraining Acuuarcy 96.983% \tValidation Acuuarcy 65.625%\n",
      "Epoch: 29 \tTraining Loss: 0.02116149 \tValidation Loss 0.24802037 \tTraining Acuuarcy 97.414% \tValidation Acuuarcy 78.125%\n",
      "Epoch: 30 \tTraining Loss: 0.02535141 \tValidation Loss 0.23811188 \tTraining Acuuarcy 97.414% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 31 \tTraining Loss: 0.01702832 \tValidation Loss 0.21401243 \tTraining Acuuarcy 97.845% \tValidation Acuuarcy 78.125%\n",
      "Epoch: 32 \tTraining Loss: 0.01773023 \tValidation Loss 0.33771536 \tTraining Acuuarcy 96.983% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 33 \tTraining Loss: 0.05002163 \tValidation Loss 0.39620907 \tTraining Acuuarcy 91.810% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 34 \tTraining Loss: 0.02372569 \tValidation Loss 0.26561269 \tTraining Acuuarcy 98.707% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 35 \tTraining Loss: 0.01404471 \tValidation Loss 0.21500354 \tTraining Acuuarcy 98.276% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 36 \tTraining Loss: 0.02347382 \tValidation Loss 0.20402889 \tTraining Acuuarcy 96.983% \tValidation Acuuarcy 81.250%\n",
      "Epoch: 37 \tTraining Loss: 0.03362687 \tValidation Loss 0.27077540 \tTraining Acuuarcy 95.259% \tValidation Acuuarcy 65.625%\n",
      "Epoch: 38 \tTraining Loss: 0.03459972 \tValidation Loss 0.24979220 \tTraining Acuuarcy 95.690% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 39 \tTraining Loss: 0.01033554 \tValidation Loss 0.21202211 \tTraining Acuuarcy 98.707% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 40 \tTraining Loss: 0.02208827 \tValidation Loss 0.34392992 \tTraining Acuuarcy 96.983% \tValidation Acuuarcy 65.625%\n",
      "Epoch: 41 \tTraining Loss: 0.01472427 \tValidation Loss 0.28328920 \tTraining Acuuarcy 98.276% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 42 \tTraining Loss: 0.02236172 \tValidation Loss 0.19747661 \tTraining Acuuarcy 96.121% \tValidation Acuuarcy 78.125%\n",
      "Epoch: 43 \tTraining Loss: 0.00644986 \tValidation Loss 0.31813289 \tTraining Acuuarcy 99.569% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 44 \tTraining Loss: 0.01128944 \tValidation Loss 0.37552723 \tTraining Acuuarcy 98.707% \tValidation Acuuarcy 65.625%\n",
      "Epoch: 45 \tTraining Loss: 0.01959876 \tValidation Loss 0.39614005 \tTraining Acuuarcy 97.845% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 46 \tTraining Loss: 0.01917963 \tValidation Loss 0.30577109 \tTraining Acuuarcy 96.983% \tValidation Acuuarcy 68.750%\n",
      "Epoch: 47 \tTraining Loss: 0.00992239 \tValidation Loss 0.20225856 \tTraining Acuuarcy 98.707% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 48 \tTraining Loss: 0.01247699 \tValidation Loss 0.30500202 \tTraining Acuuarcy 97.845% \tValidation Acuuarcy 75.000%\n",
      "Epoch: 49 \tTraining Loss: 0.00998274 \tValidation Loss 0.23905500 \tTraining Acuuarcy 97.845% \tValidation Acuuarcy 65.625%\n",
      "Epoch: 50 \tTraining Loss: 0.01398365 \tValidation Loss 0.35967632 \tTraining Acuuarcy 98.276% \tValidation Acuuarcy 65.625%\n",
      "===================================Training Finished===================================\n"
     ]
    }
   ],
   "source": [
    "Train(epochs, train_loader, val_loader, criterion, optmizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 67 %\n"
     ]
    }
   ],
   "source": [
    "testcsv_file = 'test_ckplus.csv'\n",
    "test_img_dir = 'ck_new/'\n",
    "test_dataset= Jaffe_Dataset(csv_file=testcsv_file, img_dir = test_img_dir, datatype = 'test', transform = transformation)\n",
    "test_loader= DataLoader(test_dataset,batch_size=63,shuffle = True,num_workers=0)\n",
    "\n",
    "total = []\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data,labels in test_loader:\n",
    "        output = net(data)\n",
    "        pred = F.softmax(output,dim=1)\n",
    "        result = torch.argmax(pred,1)\n",
    "        wrong = torch.where(result != labels,torch.tensor([1.]),torch.tensor([0.])) \n",
    "        acc = 1-(torch.sum(wrong)/70)\n",
    "        total.append(acc.item())\n",
    "\n",
    "    print('Accuracy of the network on the test images: %d %%' % (100 * np.mean(total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(preds, labels, conf_matrix):\n",
    "#     preds = torch.argmax(preds, 1)\n",
    "    for p, t in zip(preds, labels):\n",
    "        conf_matrix[p, t] += 1\n",
    "    return conf_matrix\n",
    "\n",
    "Emotion_kinds = 7\n",
    "conf_matrix = torch.zeros(Emotion_kinds, Emotion_kinds)\n",
    "\n",
    "\n",
    "            \n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data,labels in test_loader:\n",
    "        output = net(data)\n",
    "        pred = F.softmax(output,dim=1)\n",
    "        result = torch.argmax(pred,1)\n",
    "        conf_matrix = confusion_matrix(result, labels, conf_matrix)\n",
    "\n",
    "Emotion=7#这个数值是具体的分类数，大家可以自行修改\n",
    "labels = ['angry', 'contempt', 'disgust', 'fear', 'happy','sadness','surprise']#每种类别的标签\n",
    "# 0=anger, 1=contempt, 2=disgust, 3=fear, 4=happy, 5=sadness, 6=surprise\n",
    "# 显示数据\n",
    "plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "# 在图中标注数量/概率信息\n",
    "thresh = conf_matrix.max() / 2\t#数值颜色阈值，如果数值超过这个，就颜色加深。\n",
    "for x in range(Emotion_kinds):\n",
    "    for y in range(Emotion_kinds):\n",
    "        # 注意这里的matrix[y, x]不是matrix[x, y]\n",
    "        info = int(conf_matrix[y, x])\n",
    "        plt.text(x, y, info,\n",
    "                 verticalalignment='center',\n",
    "                 horizontalalignment='center',\n",
    "                 color=\"white\" if info > thresh else \"black\")\n",
    "                 \n",
    "plt.tight_layout()#保证图不重叠\n",
    "plt.yticks(range(Emotion_kinds), labels)\n",
    "plt.xticks(range(Emotion_kinds), labels,rotation=45)#X轴字体倾斜45°\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
